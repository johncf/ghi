#!/usr/bin/env python3

import argparse
import math
import os
import platform
import re
import shutil
import stat
import subprocess
import tarfile
from dataclasses import dataclass
from functools import cache
from pathlib import Path

import requests

_TARGET_DIR = Path.home() / ".local" / "bin"
GHI_TARGET_DIR = os.environ.get("GHI_TARGET_DIR", str(_TARGET_DIR))
GITHUB_API_HEADERS = {"Accept": "application/vnd.github+json", "X-GitHub-Api-Version": "2022-11-28"}


def cli():
    parser = argparse.ArgumentParser()
    parser.add_argument("--target", type=str, default=GHI_TARGET_DIR, help=f"install target directory [default: {GHI_TARGET_DIR}]")
    subparsers = parser.add_subparsers(required=True)

    # get command
    kws, neg_kws = get_platform_keywords()
    parser_get = subparsers.add_parser("get", help="download a github release")
    parser_get.add_argument("repo", type=str, help="name or url of the repo")
    parser_get.add_argument("-k", action='append', dest="kws", help=f"additional keywords for ranking the release assets [base: {kws}]")
    parser_get.add_argument("-K", action='append', dest="neg_kws", help=f"additional negative keywords for ranking the release assets [base: {neg_kws}]")
    parser_get.add_argument("-t", "--tag", default=None, help=f"github release tag [default: latest]")
    parser_get.set_defaults(func=cli_get)

    # ls command
    parser_ls = subparsers.add_parser("ls", help="list releases")
    parser_ls.add_argument("repo", type=str, help="name or url of the repo")
    parser_ls.add_argument("-n", type=int, default=10, help="number of releases to list [default: 10]")
    parser_ls.set_defaults(func=cli_ls)

    args = parser.parse_args()
    args.func(args)


def cli_get(args):
    owner, repo = process_github_url(args.repo)
    version = "latest" if args.tag is None else f"tags/{args.tag}"
    keywords, neg_keywords = map(list, get_platform_keywords())
    keywords.extend(args.kws or [])
    neg_keywords.extend(args.neg_kws or [])

    # fetch release data
    response = requests.get(
        f"https://api.github.com/repos/{owner}/{repo}/releases/{version}",
        headers=GITHUB_API_HEADERS,
    )
    if not response.ok:
        print(f"Error {response.status_code}:", response.text)
        return
    data = response.json()
    info = [key for key in ["draft", "prerelease"] if data[key]]
    info_str = f"({', '.join(info)})" if info else ""
    print("Release:", data["name"], info_str)
    print("    tag:", data['tag_name'])
    print("    published:", data['published_at'])

    # extract asset metadata
    assets = [
        Asset(name=asset["name"], size=asset["size"], dl_url=asset["browser_download_url"])
        for asset in data["assets"]
        if not asset["name"].endswith(".sha256")
    ]
    asset_names = [asset.name for asset in assets]
    if len(asset_names) == 0:
        print("The release contains no downloadable assets")
        return
    ranked_indices = rerank_list(asset_names, keywords, neg_keywords)

    # download a user-chosen asset
    ranked_details = [f"{assets[i].name} ({size_fmt(assets[i].size)})"
                      for i in ranked_indices]
    chosen_index = pick("Choose an asset", ranked_details)
    chosen_asset = assets[ranked_indices[chosen_index]]
    output_path = Path(chosen_asset.name)  # TODO temp/cache directory
    if output_path.exists():
        print("Skipping download; using cached file:", output_path)
    else:
        print("Downloading:", chosen_asset.dl_url)
        dl = Downloader()
        dl.download(output_path, chosen_asset.dl_url)

    # extract the pre-built binary from the downloaded asset
    # TODO support zip, and uncompressed files (e.g. exe)
    print("Reading:", output_path)
    with tarfile.open(output_path, "r") as tar:
        for tarinfo in tar:
            ftyp = tartype2chr(tarinfo.type)
            mode = stat.filemode(tarinfo.mode)[1:]
            print(f"{ftyp}{mode} {size_fmt(tarinfo.size, True)} {tarinfo.name}")
    # TODO unified extractor API:
    # with compressed_open(output_path) as extractor:
    #     members = extractor.get_details()  # list of pairs?
    #     chosen_index = pick([details for _, details in members])
    #     exec_name = input("Executable name [default={...}] ")
    #     extractor.extract(members[chosen_index][0], exec_name)


def cli_ls(args):
    owner, repo = process_github_url(args.repo)
    response = requests.get(
        f"https://api.github.com/repos/{owner}/{repo}/releases?per_page={args.n}",
        headers=GITHUB_API_HEADERS,
    )
    if not response.ok:
        print(f"Error {response.status_code}:", response.text)
        return
    data = response.json()
    for release in data:
        print(release["tag_name"], "::", release["published_at"])


@cache
def get_platform_keywords():
    keywords, neg_keywords = [], []

    system = platform.system().lower()
    append_to = keywords if system == "linux" else neg_keywords
    append_to.extend(["linux", "unknown"])
    append_to = keywords if system == "windows" else neg_keywords
    append_to.extend(["windows", "msvc", ".exe"])
    append_to = keywords if system == "darwin" else neg_keywords
    append_to.extend(["apple", "darwin", ".pkg"])
    kwlen = len(keywords)
    if kwlen == 0:
        keywords.append(system)

    machine = platform.machine().lower()
    append_to = keywords if machine in {"x86_64", "amd64"} else neg_keywords
    append_to.extend(["x86_64", "amd64", "x64"])
    if len(keywords) == kwlen:
        keywords.append(machine)
    return tuple(keywords), tuple(neg_keywords)  # tuple for immutability


def process_github_url(repo_url):
    if not repo_url.startswith("https:"):
        repo_url = f"https://github.com/{repo_url}"
    pattern = r"^https://github\.com/([^/]+)/([^/]+)"
    match = re.match(pattern, repo_url)
    if match:
        return match.group(1), match.group(2)
    else:
        raise ValueError(f"Invalid Github URL: {repo_url}")


def rerank_list(names, keywords, neg_keywords=None):
    if neg_keywords == None:
        neg_keywords = []
    weights = [sum(nkw in name for nkw in neg_keywords) - sum(kw in name for kw in keywords) for name in names]
    return [index for _, _, index in sorted(zip(weights, names, range(len(names))))]


def pick(prompt, choices):
    if len(choices) == 0:
        raise ValueError("There is not even an illusion of choice")
    print()
    padwidth = int(math.log10(len(choices) - 1)) + 2
    for index, choice in reversed(list(enumerate(choices))):
        print(f" ({index:{padwidth}})", choice)
    print()
    index_str = input(f"{prompt} [default=0] ")
    if index_str == "":
        index_str = "0"
    if not index_str.isdigit():
        raise ValueError(f"Not a positive integer: {index_str}")
    index = int(index_str)
    if index >= len(choices):
        raise ValueError(f"Thinking outside the box is not appreciated: {index} > {len(choices) - 1}")
    return index


# based on https://stackoverflow.com/a/1094933/2849934
def size_fmt(num_bytes: int, pad=False) -> str:
    if num_bytes < 1024:
        return f"{num_bytes} B" if not pad else f"{num_bytes:6} B  "
    for unit in ["KiB", "MiB", "GiB", "TiB"]:
        num_bytes /= 1024
        if num_bytes < 1024:
            break
    return f"{num_bytes:.1f} {unit}" if not pad else f"{num_bytes:6.1f} {unit}"


def tartype2chr(tartype):
    typemap = {tarfile.REGTYPE: "-", tarfile.AREGTYPE: "-", tarfile.LNKTYPE: "-", tarfile.CONTTYPE: "-", tarfile.SYMTYPE: "l", tarfile.DIRTYPE: "d"}
    return typemap[tartype] if tartype in typemap else "?"


@dataclass(kw_only=True)
class Asset:
    name: str
    size: int
    dl_url: str


class Downloader:
    __slots__ = ("download",)

    def __init__(self):
        if shutil.which("wget") is not None:
            self.download = self._wget
        elif shutil.which("curl") is not None:
            self.download = self._curl
        else:
            self.download = self._pyreq

    def _wget(self, outfile, url):
        subprocess.run(["wget", "-nv", "-O", str(outfile), url])

    def _curl(self, outfile, url):
        subprocess.run(["curl", "-sS", "-fLo", str(outfile), url])

    def _pyreq(self, outfile, url):
        with requests.get(url, stream=True) as resp:
            resp.raise_for_status()
            with open(outfile, "wb") as f:
                for chunk in resp.iter_content(chunk_size=1 << 18):
                    f.write(chunk)


if __name__ == "__main__":
    cli()
