#!/usr/bin/env python3

import argparse
import bz2
import gzip
import math
import os
import platform
import re
import shutil
import stat
import subprocess
import tarfile
import time
from collections import namedtuple
from contextlib import contextmanager
from functools import cache
from pathlib import Path
from zipfile import ZipFile

import requests

_TARGET_DIR = Path.home() / ".local" / "bin"
GHI_TARGET_DIR = os.environ.get("GHI_TARGET_DIR", str(_TARGET_DIR))
GITHUB_API_HEADERS = {"Accept": "application/vnd.github+json", "X-GitHub-Api-Version": "2022-11-28"}
_TAR_PATTERN = r"\.tar\.(gz|bz2|xz)$|\.(tar|tgz)$"
_EXTRACTOR_PATTERN = _TAR_PATTERN + r"|\.(zip|gz|bz2)$"
_CHECKSUM_PATTERN = r"\.sha(256|512)(sum)?$"


def cli():
    parser = argparse.ArgumentParser()
    parser.add_argument("--target", type=str, default=GHI_TARGET_DIR, help=f"install target directory [default: {GHI_TARGET_DIR}]")
    subparsers = parser.add_subparsers(required=True)

    # get command
    kws, neg_kws = get_platform_keywords()
    parser_get = subparsers.add_parser("get", help="download a github release")
    parser_get.add_argument("repo", type=str, help="name or url of the repo")
    parser_get.add_argument("-k", action='append', dest="kws", help=f"additional keywords for ranking the release assets [base: {kws}]")
    parser_get.add_argument("-K", action='append', dest="neg_kws", help=f"additional negative keywords for ranking the release assets [base: {neg_kws}]")
    parser_get.add_argument("-t", "--tag", default=None, help=f"github release tag [default: latest]")
    parser_get.set_defaults(func=cli_get)

    # ls command
    parser_ls = subparsers.add_parser("ls", help="list releases")
    parser_ls.add_argument("repo", type=str, help="name or url of the repo")
    parser_ls.add_argument("-n", type=int, default=10, help="number of releases to list [default: 10]")
    parser_ls.set_defaults(func=cli_ls)

    args = parser.parse_args()
    args.func(args)


def cli_get(args):
    owner, repo = process_github_url(args.repo)
    version = "latest" if args.tag is None else f"tags/{args.tag}"
    keywords, neg_keywords = map(list, get_platform_keywords())
    keywords.extend(args.kws or [])
    neg_keywords.extend(args.neg_kws or [])

    # fetch release data
    response = requests.get(
        f"https://api.github.com/repos/{owner}/{repo}/releases/{version}",
        headers=GITHUB_API_HEADERS,
    )
    if not response.ok:
        print(f"Error {response.status_code}:", response.text)
        return
    data = response.json()
    info = [key for key in ["draft", "prerelease"] if data[key]]
    info_str = f"({', '.join(info)})" if info else ""
    print("Release:", data["name"], info_str)
    print("    tag:", data['tag_name'])
    print("    published:", data['published_at'])

    # extract asset metadata
    assets = [
        Asset(name=asset["name"], size=asset["size"], dl_url=asset["browser_download_url"])
        for asset in data["assets"]
        if re.search(_CHECKSUM_PATTERN, asset["name"], re.IGNORECASE) is None
    ]
    if len(assets) == 0:
        print("The release contains no relevant assets")
        return

    # let the user pick an asset, and download it
    asset_names = [asset.name for asset in assets]
    ranked_indices = rerank_list(asset_names, keywords, neg_keywords)
    ranked_details = [f"{assets[i].name} ({size_fmt(assets[i].size)})" for i in ranked_indices]
    chosen_index = pick("Choose an asset", ranked_details)
    chosen_asset = assets[ranked_indices[chosen_index]]
    output_path = Path(chosen_asset.name)  # TODO temp/cache directory
    if output_path.exists():
        print("Skipping download; using cached file:", output_path)
        time.sleep(0.6)
    else:
        print("Downloading:", chosen_asset.dl_url)
        dl = Downloader()
        dl.download(output_path, chosen_asset.dl_url)

    # extract the pre-built binary from the downloaded asset
    print("Reading:", output_path)
    if re.search(_EXTRACTOR_PATTERN, output_path.name) is None:
        print("Unsupported file:", output_path)
        return
    with compressed_open(output_path) as extractor:
        details_list = extractor.get_details()
        if details_list is not None:
            chosen_index = pick("Choose the executable", [details for _, details in details_list])
            chosen_name = details_list[chosen_index][0]
        else:
            chosen_name = output_path.stem
        target_name = Path(chosen_name).name.split("_", 1)[0]
        exec_name = input(f"Target executable name [default={target_name}] ") or target_name
        target_path = Path(args.target) / exec_name
        print("Extracting to:", target_path)
        extractor.extract(chosen_name, target_path)

    # remove the downloaded file
    print("Downloaded file no longer necessary:", output_path)
    remove_ok = input("Delete the downloaded file? [Y/n] ")
    if remove_ok in {"Y", "y", ""}:
        os.remove(output_path)
        print("Deleted", output_path)


def cli_ls(args):
    owner, repo = process_github_url(args.repo)
    response = requests.get(
        f"https://api.github.com/repos/{owner}/{repo}/releases?per_page={args.n}",
        headers=GITHUB_API_HEADERS,
    )
    if not response.ok:
        print(f"Error {response.status_code}:", response.text)
        return
    data = response.json()
    for release in data:
        print(release["tag_name"], "::", release["published_at"])


@cache
def get_platform_keywords():
    keywords, neg_keywords = [], []

    system = platform.system().lower()
    append_to = keywords if system == "linux" else neg_keywords
    append_to.extend(["linux", "unknown"])
    append_to = keywords if system == "windows" else neg_keywords
    append_to.extend(["windows", "msvc", ".exe"])
    append_to = keywords if system == "darwin" else neg_keywords
    append_to.extend(["apple", "darwin", ".pkg"])
    kwlen = len(keywords)
    if kwlen == 0:
        keywords.append(system)

    machine = platform.machine().lower()
    append_to = keywords if machine in {"x86_64", "amd64"} else neg_keywords
    append_to.extend(["x86_64", "amd64", "x64"])
    if len(keywords) == kwlen:
        keywords.append(machine)
    return tuple(keywords), tuple(neg_keywords)  # tuple for immutability


def process_github_url(repo_url):
    if not repo_url.startswith("https:"):
        repo_url = f"https://github.com/{repo_url}"
    pattern = r"^https://github\.com/([^/]+)/([^/]+)"
    match = re.match(pattern, repo_url)
    if match:
        return match.group(1), match.group(2)
    else:
        raise ValueError(f"Invalid Github URL: {repo_url}")


def rerank_list(names, keywords, neg_keywords=None):
    if neg_keywords == None:
        neg_keywords = []
    weights = [sum(nkw in name.lower() for nkw in neg_keywords) - sum(kw in name.lower() for kw in keywords) for name in names]
    return [index for _, _, index in sorted(zip(weights, names, range(len(names))))]


def pick(prompt, choices):
    if len(choices) == 0:
        raise ValueError("Not even an illusion of choice")
    print()
    padwidth = int(math.log10(max(1, len(choices) - 1))) + 2
    for index, choice in reversed(list(enumerate(choices))):
        print(f" ({index:{padwidth}})", choice)
    print()
    index_str = ""
    if len(choices) > 1:
        index_str = input(f"{prompt} [default=0] ")
    else:
        print(f"{prompt} [default=0] (auto-selected the only choice)")
    if index_str == "":
        index_str = "0"
    if not index_str.isdigit():
        raise ValueError(f"Not a positive integer: {index_str}")
    index = int(index_str)
    if index >= len(choices):
        raise ValueError(f"Thinking outside the box: {index} > {len(choices) - 1}")
    return index


# based on https://stackoverflow.com/a/1094933/2849934
def size_fmt(num_bytes: int, pad=False) -> str:
    if num_bytes < 1024:
        return f"{num_bytes} B" if not pad else f"{num_bytes:6} B  "
    for unit in ["KiB", "MiB", "GiB", "TiB"]:
        num_bytes /= 1024
        if num_bytes < 1024:
            break
    return f"{num_bytes:.1f} {unit}" if not pad else f"{num_bytes:6.1f} {unit}"


@contextmanager
def compressed_open(filepath: str | Path):
    filepath = Path(filepath)
    if re.search(_TAR_PATTERN, filepath.name) is not None:
        with tarfile.open(filepath) as tarobj:
            yield TarExtractor(tarobj)
    elif filepath.name.endswith(".zip"):
        with ZipFile(filepath) as zipobj:
            yield ZipExtractor(zipobj)
    elif filepath.name.endswith(".gz"):
        with gzip.open(filepath) as fileobj:
            yield IdentityExtractor(fileobj)
    elif filepath.name.endswith(".bz2"):
        with bz2.open(filepath) as fileobj:
            yield IdentityExtractor(fileobj)
    else:
        raise ValueError(f"Unsupported file: {filepath}")


def tartype2chr(tartype):
    typemap = {tarfile.REGTYPE: "-", tarfile.AREGTYPE: "-", tarfile.SYMTYPE: "l", tarfile.DIRTYPE: "d"}
    return typemap[tartype] if tartype in typemap else "?"


# see https://stackoverflow.com/a/14105527/2849934
def chmod_a_plus_x(fileno):
    mode = os.fstat(fileno).st_mode
    mode |= 0o111  # executable bits
    os.fchmod(fileno, mode & 0o7777)


class TarExtractor:
    def __init__(self, tarobj: tarfile.TarFile):
        self._tar = tarobj

    def get_details(self):
        # the largest file is usually the file of interest
        tarinfos = sorted(self._tar.getmembers(), key=lambda ti: -ti.size)
        details_list = []
        for tarinfo in tarinfos:
            ftyp = tartype2chr(tarinfo.type)
            if ftyp != "-": continue  # skip non-regular files
            mode = stat.filemode(tarinfo.mode)[1:]
            details_list.append((tarinfo.name, f"{ftyp}{mode} {size_fmt(tarinfo.size, True)} {tarinfo.name}"))
        return details_list

    def extract(self, member_name, target_path: Path | str):
        target_path = Path(target_path)
        member = self._tar.getmember(member_name)
        member.name = target_path.name
        if not member.isreg():
            raise RuntimeError("Not a regular file")
        # TODO use filter="data" once Python 3.12 becomes common
        self._tar.extract(member, path=target_path.parent)


class ZipExtractor:
    def __init__(self, zipobj: ZipFile):
        self._zip = zipobj

    def get_details(self):
        # the largest file is usually the file of interest
        zipinfos = sorted(self._zip.infolist(), key=lambda zi: -zi.file_size)
        details_list = []
        for zipinfo in zipinfos:
            if zipinfo.is_dir(): continue  # skip directories
            mode = stat.filemode(zipinfo.external_attr >> 16)
            details_list.append((zipinfo.filename, f"{mode} {size_fmt(zipinfo.file_size, True)} {zipinfo.filename}"))
        return details_list

    def extract(self, member_name, target_path: Path | str):
        with self._zip.open(member_name) as zfd:
            with open(target_path, "wb") as outfd:
                shutil.copyfileobj(zfd, outfd)
                chmod_a_plus_x(outfd.fileno())


class IdentityExtractor:
    def __init__(self, fileobj):
        self._fd = fileobj

    def get_details(self):
        return None

    def extract(self, _, target_path: Path | str):
        with open(target_path, "wb") as outfd:
            shutil.copyfileobj(self._fd, outfd)
            chmod_a_plus_x(outfd.fileno())


Asset = namedtuple("Asset", ["name", "size", "dl_url"])


class Downloader:
    __slots__ = ("download",)

    def __init__(self):
        if shutil.which("wget") is not None:
            self.download = self._wget
        elif shutil.which("curl") is not None:
            self.download = self._curl
        else:
            self.download = self._pyreq

    def _wget(self, outfile, url):
        subprocess.run(["wget", "-nv", "-O", str(outfile), url])

    def _curl(self, outfile, url):
        subprocess.run(["curl", "-sS", "-fLo", str(outfile), url])

    def _pyreq(self, outfile, url):
        with requests.get(url, stream=True) as resp:
            resp.raise_for_status()
            with open(outfile, "wb") as f:
                for chunk in resp.iter_content(chunk_size=1 << 18):
                    f.write(chunk)


if __name__ == "__main__":
    cli()
